{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wfL6laKEEL8M"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KepxsuEXn_Y-",
        "outputId": "06deb6b3-fabe-4266-a218-dbaae04c6228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Names have been finalized and saved to Names.txt.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "'''\n",
        "Loads a pre-trained SpaCy NLP model named en_core_web_sm,\n",
        "which is optimized for English.\n",
        "This model is capable of performing various NLP tasks,\n",
        "including Named Entity Recognition (NER) used in this code.\n",
        "'''\n",
        "\n",
        "def apply_model(input_file, output_file):\n",
        "    with open(input_file, \"r\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Load the pre-trained NER model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "    # Process the text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract entities identified as PERSON\n",
        "    names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_names = list(set(names))\n",
        "\n",
        "    # Use the output_file parameter to create or overwrite the file\n",
        "    with open(output_file, 'w') as file:\n",
        "        for name in unique_names:\n",
        "            file.write(name + \"\\n\")\n",
        "\n",
        "def clean_names(input_file, output_file):\n",
        "    with open(input_file, \"r\") as file:\n",
        "        names = file.readlines()\n",
        "\n",
        "    cleaned_names = set()\n",
        "    for name in names:\n",
        "        # Find the pattern where 'View' might appear and remove everything after it\n",
        "        cleaned_name = re.sub(r'View.*', '', name).strip()\n",
        "\n",
        "        # the expression matches any character that is not a:\n",
        "        # letter, number, underscore, whitespace, or hyphen.\n",
        "        cleaned_name = re.sub(r'[^\\w\\s-]', '', cleaned_name).strip()\n",
        "\n",
        "        name_parts = cleaned_name.split()\n",
        "\n",
        "        # If there's only a first name, remove it\n",
        "        if len(name_parts) == 1:\n",
        "            continue\n",
        "\n",
        "        # If a potential middle name is present, remove it\n",
        "        if len(name_parts) == 3:\n",
        "            cleaned_name = name_parts[0] + ' ' + name_parts[2]\n",
        "\n",
        "        # Only add non-empty names\n",
        "        if cleaned_name:\n",
        "            cleaned_names.add(cleaned_name)\n",
        "\n",
        "    with open(output_file, \"w\") as file:\n",
        "        # Sort names before writing\n",
        "        for name in sorted(cleaned_names):\n",
        "            file.write(name + \"\\n\")\n",
        "\n",
        "\n",
        "apply_model(\"Icims.txt\", \"Names.txt\")\n",
        "clean_names(\"Names.txt\",\"Names.txt\")\n",
        "\n",
        "print(\"Names have been finalized and saved to Names.txt.\")\n",
        "\n",
        "\n",
        "# for i in range(5):\n",
        "  # apply_model(\"Names.txt\", \"Names.txt\")\n",
        "  # clean_names(\"Names.txt\",\"Names.txt\")\n",
        "  # print(\"Applied model & cleaned names - Iteration:\", i+1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# What is SpaCy?\n",
        "SpaCy is a popular open-source library for advanced natural language processing in Python. It's designed for practical, real-world tasks and is widely used for tasks like tokenization, part-of-speech tagging, named entity recognition, and more. SpaCy comes with pre-trained models for various languages, which can perform these tasks out of the box.\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "Named Entity Recognition (NER) is a process in NLP where the algorithm identifies named entities (like person names, locations, companies, quantities, etc.) in a text and classifies them into pre-defined categories. NER is crucial for extracting information from texts and is widely used in information retrieval, question answering systems, content classification, and more.\n",
        "\n",
        "# How Does SpaCy Perform NER?\n",
        "SpaCy's NER model is a part of its larger language processing pipeline. When you load a SpaCy model (like en_core_web_sm) and process a text with it, the text goes through several processing steps, one of which is named entity recognition. The NER model in SpaCy uses a deep learning approach, typically a convolutional neural network (CNN) or a transformer-based model, trained on a large annotated corpus where entities have been labeled according to their types."
      ],
      "metadata": {
        "id": "wfL6laKEEL8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vYkBOGOhLoXi"
      }
    }
  ]
}